\documentclass{article}
    % General document formatting
    \usepackage[margin=0.7in]{geometry}
    \usepackage[parfill]{parskip}
    \usepackage[utf8]{inputenc}
    
    % Related to math
    \usepackage{amsmath,amssymb,amsfonts,amsthm}
    \usepackage{multicol}
    \usepackage{commath}

    % Better enumeration support
    \usepackage{enumitem}

    % Metadata for front page
    \title{Statistical Inference Study Notes}
    \author{Jakob Gerhard Martinussen}
    \date{2019, Spring}

    % Numbered environments
    \theoremstyle{plain}
    \newtheorem{innertheorem}{Theorem}
    \newenvironment{theorem}[1]
      {\renewcommand\theinnertheorem{#1}\innertheorem}
      {\endinnertheorem}

    \theoremstyle{definition}
    \newtheorem{innerdefinition}{Definition}
    \newenvironment{definition}[1]
      {\renewcommand\theinnerdefinition{#1}\innerdefinition}
      {\endinnerdefinition}

    \theoremstyle{plain}
    \newtheorem{innerlemma}{Lemma}
    \newenvironment{lemma}[1]
      {\renewcommand\theinnerlemma{#1}\innerlemma}
      {\endinnerlemma}

    \theoremstyle{plain}
    \newtheorem{innercorollary}{Corollary}
    \newenvironment{corollary}[1]
      {\renewcommand\theinnercorollary{#1}\innercorollary}
      {\endinnercorollary}

    % Aliases
    \renewcommand{\vec}[1]{\boldsymbol{#1}}
    \newcommand{\E}[1]{\mathrm{\mathbb{E} \left[#1 \right]}}
    \newcommand{\Var}[1]{\mathrm{Var \left(#1 \right)}}
    \newcommand{\Cov}[2]{\mathrm{Cov}\left(#1, #2 \right)}

\begin{document}

\maketitle

\begin{multicols}{2}

\section{Probability Theory}

\begin{definition}{1.3.2}[Conditional Probability]
  If $A$ and $B$ are events in $S$, and $P(B) > 0$,
  then the \textit{conditional probability of $A$ given $B$},
  written $P(A | B)$ is
  \begin{equation*}
    P(A|B) = \frac{P(A \cap B)}{P(B)}.
  \end{equation*}
\end{definition}

\section{Transformations and Expectations}

\begin{theorem}{2.1.5}[] 
  Let $X$ have pdf $f_X(x)$ and let $Y = g(X)$, where $g$ is a monotone function.
  Let $\mathcal{X}$ and $\mathcal{Y}$ be defined by
  \begin{align*}
    \mathcal{X} &= \{x: f_X(x) > 0\}
    \\
    \mathcal{Y} &= \{ y: y = g(x) \text{ for some } x \in \mathcal{X} \}
  \end{align*}
  Suppose that $f_X(x)$ is continuous on $\mathcal{X}$ and that $g^{-1}(x)$
  has a continuous derivative and $\mathcal{Y}$. Then the pdf $Y$ is given by
  \[
    f_Y(y) = \begin{cases}
      f_X(g^{-1}(y)) \left| \od{}{y} g^{-1}(y) \right|, &y \in \mathcal{Y}
      \\
      0, &\text{otherwise}.
    \end{cases}
  \]
\end{theorem}

\begin{theorem}{2.3.15}[Moment generating functions for linear transformations]
  For any constants $a$ and $b$, the mgf of the random variable $aX + b$ is given by
  $$
  M_{aX + b}(t) = e^{bt} M_X(at)
  $$
\end{theorem}

\section{Common Families of Distributions}

\begin{theorem}{3.2.2}[Binomial Theorem]
  For any real numbers $x$ and $y$ and integer $n \geq 0$,
  $$
    (x + y)^n = \sum_{i = 0}^{n} \binom{n}{i} x^i y^{n-1}
  $$
\end{theorem}

\begin{definition}{3.4.1}[Exponential Family of Distributions]
  The exponential family consists of all pdfs and pmfs that can be expressed as
  $$
  f(x | \vec{\theta})
  =
  h(x) c(\vec{\theta}) \exp\left(
    \sum_{i = 1}^k w_i(\vec{\theta}) t_i(x)
  \right),
  $$
  where $h(x) \geq 0$ and $c(\vec{\theta}) \geq 0$.

  An alternative natural parametrization is
  $$
    f(x | \eta) = h(x) c^*(\eta) \exp \left( \sum_{i=1}^k \eta_i t_i(x) \right).
  $$
  The natural parameter space for the family is then given as
  $$
    \mathcal{H} = \left\{
      \eta = (\eta_1, ..., \eta_k):
        \int_{-\infty}^{\infty} h(x) \exp \left(
          \sum_{i=1}^k \eta_i t_i(x)
        \right)
    \right\},
  $$
  for which me must have
  $$
    c^*(\eta) = \left[
      \int_{-\infty}^{\infty}
        h(x) \exp \left( \sum_{i=1}^k \eta_i t_i(x) \right)
        \dif x
    \right]^{-1}.
  $$
\end{definition}

\begin{theorem}{3.4.2}[Exponential Family Statistics]
  \begin{align}
    \E{
      \sum_{i = 1}^k \pd{w_i(\vec{\theta})}{\theta_j} t_i(X)
    }
    &=
    -\pd{}{\theta_j} \log c(\vec{\theta})
    \\
    \Var{
      \sum_{i = 1}^k \pd{w_i(\vec{\theta})}{\theta_j} t_i(X)
    }
    =
    &-\pd[2]{}{\theta_j} \log c(\vec{\theta})
    \\
    &-\E{\sum_{i = 1}^k \pd[2]{w_i(\vec{\theta})}{\theta_j} t_i(X)}
  \end{align}
\end{theorem}

\begin{definition}{3.4.7}[Curved Exponential Family]
  A curved exponential family is a family of densities of the form (3.4.1)
  for which the dimension of the vector $\vec{\theta}$ is equal to $d < k$.
  If $d = k$, the family is a full exponential family.
\end{definition}

\begin{theorem}{3.5.1}
  Let $f(x)$ be any pdf and let $\mu$ and $\sigma > 0$ be any given constants. Then the function
  $$
    g(x | \mu, \sigma) = \frac{1}{\sigma} f\del{\frac{x - \mu}{\sigma}}
  $$
  is a pdf.
\end{theorem}

\begin{definition}{3.5.2}[Location Family of Distributions]
  Let $f(x)$ be any pdf. Then the family of pdfs $f(x - \mu)$,
  indexed by the parameter $\mu$, $-\infty < \mu < \infty$,
  is called the location family with standard pdf $f(x)$
  and $-\mu$ is called the location parameter of the family.
\end{definition}

\begin{definition}{3.5.4}[Scale Family of Distributions]
  Let $f(x)$ be any pdf.
  Then for any $\sigma > 0$, the family of pdfs $\frac{1}{\sigma} f (\frac{x}{\sigma})$,
  indexed by the parameter $\sigma$, is called the scale family with standard pdf $f(x)$
  and $\sigma$ is called the scale parameter of the family.
\end{definition}

\begin{definition}{3.5.4}[Location-Scale Family of Distributions]
  Let $f(x)$ be any pdf.
  Then for any $\mu$, $-\infty < \mu < \infty$, and any $\sigma > 0$,
  the family of pdfs $\frac{1}{\sigma} f(\frac{x - \mu}{\sigma})$,
  indexed by the parameter $(\mu, \sigma)$,
  is called the location-scale family with standard pdf $f(x)$;
  $\mu$ is called the location parameter
  and $\sigma$ is called the scale parameter of the family.
\end{definition}

\begin{definition}{3.5.4}
  Let $f(\cdot)$ be any pdf. Let $\mu$ be any real number,
  and let $\sigma$ be any positive real number.
  Then $X$ is a random variable with pdf $\frac{1}{\sigma} f\del{\frac{x - \mu}{\sigma}}$
  if and only if there exists a random variable $Z$ with pdf $f(z)$ and $X = \sigma Z + \mu$.
\end{definition}

\begin{definition}{3.5.7}
  Let $Z$ be a random variable with pdf $f(z)$. Suppose $\E{Z}$ and $\Var{Z}$ exist.
  If $X$ is a random variable with pdf $\frac{1}{\sigma} f\del{\frac{x - \mu}{\sigma}}$,
  then
  $$
    \E{X} = \sigma \E{Z} + \mu,\text{ and } \Var{X} = \sigma^2 \Var{Z}.
  $$
  In particular, if $\E{Z} = 0$ and $\Var{Z} = 1$, then $\E{X} = \mu$ and $\Var{X} = \sigma^2$.
\end{definition}

\begin{theorem}{3.6.1}[Chebychev's Inequality]
  Let $X$ be a random variable and let $g(x)$ be a nonnegative function. Then, for any $r > 0$,
  $$
  P\del{g(X) \geq r} \leq \frac{\E{g(X)}}{r}.
  $$

  This can be used in order to find the following bounds
  \begin{align}
    P\del{\left| X - \mu \right| \geq t\sigma} &\leq \frac{1}{t^2}
    \\
    P\del{\left| X - \mu \right| < t\sigma} &\geq 1 - \frac{1}{t^2}
  \end{align}
\end{theorem}

\section{Multiple Random Variables}

\begin{definition}{4.1.1}[n-dimensional random vector]
  An \textit{n-dimensional random vector} is a function from a sample space $S$ into $\mathfrak{R}^n$,
  $n$-dimensional Euclidean space.
\end{definition}

\begin{definition}{4.1.3}[Joint probability mass function]
  Let $(X, Y)$ be a discrete bivariate random vector.
  Then the function $f(x, y)$ from $\mathfrak{R}^2$ into $\mathfrak{R}$ defined by
  $f(x, y) = P(X = x, Y = y)$ is called the \textit{joint probability mass function} or
  \textit{joint pmf} of $(X, Y)$. If it is necessary to stress the fact that $f$ is the
  joint pmf of the vector $(X, Y)$ rather than som other vector, the notation $f_{X,Y}(x, y)$
  will be used.
\end{definition}

\begin{theorem}{4.1.6}[Marginal distribution]
  Let $(X, Y)$ be a discrete bivariate random vector with joint pmf $f_{X,Y}(x,y)$.
  Then the marginal pmfs of $X$ and $Y$, $f_{X}(x) = P(X = x)$ and $f_{Y}(y) = P(Y = y)$,
  are given by
  \begin{equation*}
    f_{X}(x) = \sum_{y \in \mathfrak{R}} f_{X,Y}(x,y) 
    ~~~\text{ and }~~~
    f_{Y}(y) = \sum_{x \in \mathfrak{R}} f_{X,Y}(x,y) 
  \end{equation*}
\end{theorem}

\begin{definition}{4.1.10}[Joint probability density function]
  A function $f(x, y)$ from $\mathfrak{R}^2$ into $\mathfrak{R}$ is called a
  \textit{joint probability density function} or \textit{joint pdf of the continuous bivariate
  random vector} $(X, Y)$ if, for every $A \subset \mathfrak{R}^2$,
  \begin{equation*}
    P((X,Y) \in A) = \int \limits_{A} \int f(x,y) \dif{x} \dif{y}
  \end{equation*}
\end{definition}

\begin{definition}{4.2.1}[Conditional PMF]
  Let $(X,Y)$ be a discrete bivariate random vector with joint pmf $f(x,y)$ and marginal pmfs
  $f_{X}(x)$ and $f_{Y}(y)$. For any $x$ such that $P(X=x) = f_{X}(x) > 0$,
  the \textit{conditional pmf of $Y$ given that $X = x$} is the function of $y$
  denoted by $f(y | x)$ and defined by
  \begin{equation*}
    f(y|x) = P(Y = y | X = x) = \frac{f(x, y)}{f_{X}(x)}.
  \end{equation*}
  For any $y$ such that $P(Y=y) = f_{Y}(y) > 0$,
  the \textit{conditional pmf of $X$ given that $Y = y$} is the function of $x$
  denoted by $f(x | y)$ and defined by
  \begin{equation*}
    f(x|y) = P(X = x | Y = y) = \frac{f(x, y)}{f_{Y}(y)}.
  \end{equation*}
\end{definition}

\begin{definition}{4.2.1}[Conditional PDF]
  Let $(X,Y)$ be a continuous bivariate random vector with joint pdf $f(x,y)$ and marginal pdfs
  $f_{X}(x)$ and $f_{Y}(y)$. For any $x$ such that $f_{X}(x) > 0$,
  the \textit{conditional pdf of $Y$ given that $X = x$} is the function of $y$
  denoted by $f(y | x)$ and defined by
  \begin{equation*}
    f(y|x) = \frac{f(x, y)}{f_{X}(x)}.
  \end{equation*}
  For any $y$ such that $f_{Y}(y) > 0$,
  the \textit{conditional pmf of $X$ given that $Y = y$} is the function of $x$
  denoted by $f(x | y)$ and defined by
  \begin{equation*}
    f(x|y) = \frac{f(x, y)}{f_{Y}(y)}.
  \end{equation*}
\end{definition}

\begin{definition}{4.2.5}[Independent Random Variables]
  Let $(X, Y)$ be a bivariate random vector with joint pdf or pmf $f(x, y)$
  and marginal pdfs or pmfs $f_X(x)$ and $f_Y(y)$.
  Then $X$ and $Y$ are called \textit{independent random variables} if,
  for every $x \in \mathfrak{R}$ and $y \in \mathfrak{R}$,
  \begin{equation}
    f(x, y) = f_X(x) f_Y(y).
  \end{equation}
\end{definition}

\begin{lemma}{4.2.7}[Verifying Independence]
  Let $(X,Y)$ be a bivariate  random vector with joint pdf or pmf $f(x,y)$.
  Then $X$ and $Y$ are independent random variables if and only if there exist functions
  $g(x)$ and $h(y)$ such that, for every $x \in \mathfrak{R}$ and $y \in \mathfrak{R}$,
  \begin{equation*}
    f(x, y) = g(x) h(y)
  \end{equation*}
\end{lemma}

\begin{theorem}{4.2.10}[Independent Events and Expectations]
  Let $X$ and $Y$ be independent random variables.
  \begin{enumerate}[label=\alph*.]
    \item For any $A \subset \mathfrak{R}$ and $B \subset \mathfrak{R}$,
      $P(X \in A, Y \in B) = P(X \in A) P(Y \in B)$, that is, the events
      $\{X \in A\}$ and $\{Y \in B\}$ are independent events.
    \item Let $g(x)$ be a function only of $x$ and $h(y)$ be a function only of $y$. Then
      \begin{equation*}
        \E{g(X) h(Y)} = \E{g(X)} \E{h(Y)}.
      \end{equation*}
  \end{enumerate}
\end{theorem}

\begin{theorem}{4.2.12}[MGF of Sum of Independent Variables]
  Let $X$ and $Y$ be independent random variables with moment generating functions
  $M_{X}(t)$ and $M_{Y}(t)$. Then the moment generating function of the random variable
  $Z = X + Y$ is given by
  \begin{equation*}
    M_Z(t) = M_X(t) M_Y(t).
  \end{equation*}
\end{theorem}

\begin{theorem}{4.2.14}[Sum of Two Random Normal Variables]
  Let $X \sim \mathcal{N}(\mu, \sigma^{2})$ and $Y \sim \mathcal{N}(\gamma, \tau^{2})$
  be independent normal random variables. Then the random variable $Z = X + Y$ has
  a $\mathcal{N}(\mu + \tau, \sigma^{2} + \tau^{2})$ distribution.
\end{theorem}

\begin{theorem}{4.3.2}[]
  If $X \sim \text{Poisson}(\theta)$ and $Y \sim \text{Poisson}(\lambda)$
  and $X$ and $Y$ are independent, then $X + Y \sim \text{Poisson}(\theta + \lambda)$.
\end{theorem}

\begin{theorem}{4.4.3}[Law of Double Expectation]
  If $X$ and $Y$ are any two random variables, then
  \begin{equation*}
    \E{X} = \E{\E{X | Y}},
  \end{equation*}
  provided that the expectations exist.
\end{theorem}

\begin{theorem}{4.4.7}[Conditional Variance Identity]
  For any two random variables $X$ and $Y$,
  \begin{equation*}
    \Var{X} = \E{\Var{X | Y}} + \Var{\E{X | Y}},
  \end{equation*}
  provided that the expectations exist.
\end{theorem}

\begin{definition}{4.5.1}[Covariance]
  The \textit{covariance of $X$ and $Y$} is the number defined by
  \begin{equation*}
    Cov(X, Y) = \E{(X - \mu_X)(Y - \mu_Y)}.
  \end{equation*}
\end{definition}

\begin{definition}{4.5.2}[Correlation]
  The \textit{correlation of $X$ and $Y$} is the number defined by
  \begin{equation*}
    \rho_{XY} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}.
  \end{equation*}
  The value $\rho_{XY}$ is also called the \textit{correlation coefficient}.
\end{definition}

\begin{theorem}{4.5.3}
  For any random variables $X$ and $Y$,
  \begin{equation*}
    \text{Cov}(X, Y) = \E{XY} - \mu_X \mu_Y
  \end{equation*}
\end{theorem}

\begin{theorem}{4.5.5}
  If $X$ and $Y$ are independent random variables, then $\text{Cov}(X, Y) = 0$
  and $\rho_{XY} = 0$.
\end{theorem}

\begin{theorem}{4.5.6}
  If $X$ and $Y$ are any two random variables and $a$ and $b$ are any two
  constants, then
  \begin{equation*}
    \text{Var}(aX + bY)
    =
    a^{2} \text{Var}(X) + b^{2} \text{Var}(Y) + 2ab \text{Cov}(X, Y)
  \end{equation*}
\end{theorem}

\begin{definition}{4.4.4}[Mixture Distribution]
  A random variable $X$ is said to have a \textit{mixture distribution}
  if the distribution of $X$ depends on a quantity that also has a distribution.
\end{definition}

\begin{theorem}{4.5.7}
  For any random variables $X$ and $Y$,
  \begin{enumerate}[label=\alph*.]
    \item $-1 \leq \rho_{XY} \leq 1$.
    \item $|\rho_{XY}| = 1$ if and only if there exist numbers $a \neq 0$
      and $b$ such that $P(Y = aX + b) = 1$.
      If $\rho_{XY} = 1$, then $a > 0$, and if $\rho_{XY} = -1$, then $a < 0$.
  \end{enumerate}
\end{theorem}

\begin{definition}{4.6.2}[Multinomial Distribution]
  Let $n$ and $m$ be positive integers and let $p_1, ..., p_n$ be numbers satisfying
  $0 \leq p_i \leq 1, i = 1, ..., n$ and $\sum_{i=1}^n p_i = 1$.
  Then the random vector $(X_1, ..., X_n)$ has a
  \textit{multinomial distribution with $m$ trials and cell probabilities $p_1, ..., p_n$}
  if the joint pmf of $(X_1, ..., X_n)$ is
  \begin{equation*}
    f(x_1, ..., x_n)
    = \frac{m!}{x_1!...x_n!} p_1^{x_1} ... p_n^{x_n}
    = m! \prod_{i=1}^n \frac{p_i^{x_i}}{x_i!}
  \end{equation*}
  on the set of $(x_1, ..., x_n)$ such that each $x_i$ is a nonnegative integer
  and $\sum_{i = 1}^n x_i = m$.
\end{definition}

\begin{theorem}{4.6.2}[Multinomial Theorem]
  Let $m$ and $n$ be positive integers.
  Let $\mathcal{A}$ be the set of vectors $\vec{x} = (x_1, ..., x_n)$
  such that each $x_i$ is a nonnegative integer
  and $\sum \limits_{i=1}^{n} x_i = m$. Then, for any real numbers $p_1, ..., p_n$,
  \begin{equation*}
    (p_1 + ... + p_n)^m
    =
    \sum_{x \in \mathcal{A}} \frac{m!}{x_1! ... x_n!} p_1^{x_1} ... p_n^{x_n}.
  \end{equation*}
\end{theorem}

\begin{definition}{4.6.5}
  Let $\vec{X}_1, ..., \vec{X}_n$ be random vectors with joint pdf or pmf
  $f(\vec{x}_1, ..., \vec{x}_n)$. Let $f_{\vec{X}_i}(\vec{x}_i)$ denote the
  marginal pdf or pmf of $\vec{X}_i$. Then $\vec{X_1}, ..., \vec{X_n}$
  are called \textit{mutually independent random vectors} if, for every
  $(\vec{x}_1, ..., \vec{x}_n)$,
  \begin{equation*}
    f(\vec{x}_1, ..., \vec{x}_n) = \prod_{i = 1}^{n} f_{\vec{X}_i}(\vec{x}_i).
  \end{equation*}
  If the $X_i$s are alle one-dimensional, then $X_1, ..., X_n$ are called
  \textit{mutually independent random variables}.
\end{definition}

\begin{theorem}{4.6.6}[]
  Let $X_1, ..., X_n$ be mutually independent random variables.
  Let $g_1, ..., g_n$ be real-valued functions such that $g_i(x_i)$ is a function
  only of $x_i, i = 1, ..., n$.
  Then
  \begin{equation*}
    \E{g_1(X_1) \cdot ... \cdot g_n(X_n)}
    =
  \E{g_1(X_1)} \cdot ... \cdot \E{g_n(X_n)}
  \end{equation*}
\end{theorem}

\begin{theorem}{4.6.7}[]
  Let $X_1, ..., X_n$ be mutually independent random variables with mgfs
  $M_{X_1}(t), ..., M_{X_n}(t)$. Let $Z = X_1 + ... + X_n$.
  Then the mgf of $Z$ is
  \begin{equation*}
    M_Z(t) = M_{X_1}(t) \cdot ... \cdot M_{X_n}(t).
  \end{equation*}
  In particular, if $X_1, ..., X_n$ all have the same distribution with mgf
  $M_X(t)$, then
  \begin{equation*}
    M_Z(t) = \del{M_X(t)}^n.
  \end{equation*}
\end{theorem}

\begin{corollary}{4.6.9}[]
  Let $X_1, ..., X_n$ be mutually independent random variables with mgfs
  $M_{X_1}(t), ..., M_{X_n}(t)$. Let $a_1, ..., a_n$ and $b_1, ..., b_n$
  bef fixed constants. Let $Z = (a_1 X_1 + b_1) + ... + (a_n X_n + b_n)$.
  Then the mgf of $Z$ is
  \begin{equation*}
    M_Z(t) = \del{e^{t\del{\sum b_i}}} M_{X_1}(a_1 t) \cdot ... \cdot M_{X_n}(a_n t)
  \end{equation*}
\end{corollary}

\begin{corollary}{4.6.10}[]
  Let $X_1, ..., X_n$ be mutually independent random variables with
  $X_i \sim \mathcal{N}(\mu_i, \sigma_i^{2})$. Let $a_1, ..., a_n$
  and $b_1, ..., b_n$ be fixed constants. Then
  \begin{equation*}
    Z = \sum_{i = 1}^{n} \del{a_i X_i + b_i}
    \sim
    \mathcal{N}\del{
      \sum_{i = 1}^{n} (a_i \mu_i + b_i), \sum_{i = 1}^{n} a_i^{2} \sigma_i^{2}
    }
  \end{equation*}
\end{corollary}

\begin{theorem}{4.6.11}[]
  Let $\vec{X}_1, ..., \vec{X}_n$ be random vectors.
  Then $\vec{X}_1, ..., \vec{X}_n$ are mutually independent vectors if and only if
  there exist functions $g_i(\vec{x}_i), i = 1, ..., n$, such that the joint pdf or pmf of
  $(\vec{X}_1, ..., \vec{X}_n)$ can be written as
  \begin{equation*}
    f(\vec{x}_1, ..., \vec{x}_n) = g_1(\vec{x}_1) \cdot ... \cdot g_n(\vec{x}_n).
  \end{equation*}
\end{theorem}

\begin{theorem}{4.6.12}[]
  Let $\vec{X}_1, ..., \vec{X}_n$ be independent random vectors. Let $g_i(\vec{x}_i)$
  be a function only of $\vec{x}_i, i = 1, ..., n$. Then the random variables
  $U_i = g_i(\vec{X}_i), i = 1, ..., n$, are mutually independent.
\end{theorem}

\begin{theorem}{4.6.13}[]
  Let $(X_1, ..., X_n)$ be a random vector with pdf $f(x_1, ..., x_n)$.
  Consider a new random vector $(U_1, ..., U_n)$, defined by
  $U_j = g_j(X_1, ..., X_n), j = 1, ..., n$.
  Denote the $i$th inverse by $x_1 = h_{1i}(u_1, ..., u_n)$.
  Then
  \begin{align*}
    & f_{\vec{U}}(u_1, ..., u_n)
    \\
    & ~~~~ = \sum_{i = 1}^{k} f_{\vec{X}}(h_{1i}(u_1, ..., u_n), ..., h_{ni}(u_1, ..., u_n)) |J_i|,
  \end{align*}
  where
  \begin{equation*}
    \del{J_i}_{jk} = \pd{x_i}{u_k} = \pd{h_{ji}}{u_k}.
  \end{equation*}
\end{theorem}

\begin{theorem}{4.7.3}[Cauchy-Schwarz Inequality]
  For any two random variables $X$ and $Y$,
  \begin{equation*}
    |\E{XY}| \leq \E{|XY|} \leq \sqrt{\E{|X|^2}} \sqrt{\E{|Y|^2}}
  \end{equation*}
\end{theorem}

\begin{theorem}{4.7.7}[Jensen's Inequality]
  For any random variable $X$, if $g(x)$ is a convex function, then
  \begin{equation*}
    \E{g(X)} \geq g\del{\E{X}}
  \end{equation*}
  Equality holds if and only if, for every line $a + bx$ that is tangent to $g(x)$ at $x = \E{X}$, $P(g(X) = a + bX) = 1$.
\end{theorem}

\section{Properties of a Random Sample}

\begin{definition}{5.1.1}[Random sample]
  The random variables $X_1, ..., X_n$ are called a \textit{random sample of size $n$ from the population $f(x)$} 
  if $X_1, ..., X_n$ are mutually independent random variables and the marginal pdf or pmf of each $X_i$ is the same function $f(x)$.
  Alternatively, $X_1, ..., X_n$ are called \textit{independent and identically distributed random variables with pdf or pmf $f(x)$.} 
  This is commonly abbreviated to iid random variables.
\end{definition}

\begin{definition}{5.2.1}[Statistics]
  Let $X_1, ..., X_n$ be a random sample of size $n$ from a population and let $T(x_1, ..., x_n)$ be a real-valued or vector-valued
  function whose domain includes the sample space of $(X_1, ..., X_n)$. Then the random variable or random vector
  $Y = T(X_1, ..., X_n)$ is called a \textit{statistic}. The probability distribution of a statistic $Y$ is called the
  \textit{sampling distribution of Y}.
\end{definition}

\begin{definition}{5.2.2}[Sample mean]
  The \textit{sample mean} is the arithmetic average of the values in a random sample. It is usually denoted by
  \begin{equation*}
    \bar{X} = \frac{X_1 + ... + X_n}{n} = \frac{1}{n} \sum_{i = 1}^{n} X_i. 
  \end{equation*}
\end{definition}

\begin{definition}{5.2.3}[Sample variance]
  The \textit{sample variance} is the statistic defined by
  \begin{equation*}
    S^2 = \frac{1}{n - 1} \sum_{i = 1}^{n} (X_i - \bar{X})^2.
  \end{equation*}
  The \textit{sample standard deviation} is the statistic defined by $S = \sqrt{S^2}$.
\end{definition}

\begin{theorem}{5.2.4}[Statistical properties]
  Let $x_1, ..., x_n$ be any numbers and $\bar{x} = (x_1 + ... + x_n)/n$.
  Then
  \begin{enumerate}[label=\alph*.]
    \item $\min_a \sum_{i = 1}^{n} (x_i - a)^2 = \sum_{i = 1}^{n} (x_i - \bar{x})^2$,
    \item $(n - 1)s^2 = \sum_{i = 1}^{n} (x_i - \bar{x})^2 = \sum_{i = 1}^{n} x_i^2 - n \bar{x}^2$.
  \end{enumerate}
\end{theorem}

\begin{lemma}{5.2.5}
  Let $X_1, ..., X_n$ be a random sample from a population and let $g(x)$ be a function such that
  $\E{g(X_1)}$ and $\Var{g(X_1)}$ exist. Then
  \begin{equation*}
    \E{\sum \limits_{i=1}^{n} g(X_i)} = n\left( \E{g(X_1)} \right)
  \end{equation*}
  and
  \begin{equation*}
    \Var{\sum \limits_{i=1}^{n} g(X_i)} = n\left( \Var{g(X_1)} \right)
  \end{equation*}
\end{lemma}

\begin{theorem}{5.2.6}
  Let $X_1, ..., X_n$ be a random sample from a population with mean $\mu$ and variance
  $\sigma^2 < \infty$. Then
  \begin{enumerate}[label=\alph*.]
    \item $\E{\bar{X}} = \mu$,
    \item $\Var{\bar{X}} = \frac{\sigma^{2}}{n}$,
    \item $\E{S^2} = \sigma^{2}$.
  \end{enumerate}
\end{theorem}

\begin{theorem}{5.2.7}[MGF of sample mean]
  Let $X_1, ..., X_n$ be a random sample from a population with mgf $M_X(t)$.
  Then the mgf of the sample mean is
  \begin{equation*}
    M_{\bar{X}}(t) = \left[ M_X(t/n) \right]^n
  \end{equation*}
\end{theorem}

\begin{theorem}{5.2.9}
  If $X$ and $Y$ are independent continuous random variables with pdfs $f_X(x)$ and $f_Y(y)$,
  then the pdf of $Z = X + Y$ is
  \begin{equation*}
    f_Z(z) = \int_{-\infty}^\infty f_X(w) f_Y(z-w) \dif{w}.
  \end{equation*}
\end{theorem}

\begin{theorem}{5.2.11}
  Suppose $X_1, ..., X_n$ is a random sample from a pdf or pmf $f(x | \theta)$, where
  \begin{equation*}
    f(x | \theta) = h(x) c(\theta) \exp{\left( \sum_{i = 1}^{k} w_i(\theta) t_i(x) \right)}
  \end{equation*}
  is a member of the exponential family. Define statistics $T_1, ..., T_k$ by
  \begin{equation*}
    T_i(X_1, ..., X_n) = \sum_{j = 1}^{n} t_i(X_j), ~~~~~ i = 1,...,k.
  \end{equation*}
  If the set $\{(w_1(\theta), ..., w_k(\theta)), \theta \in \Theta \}$ contains an open subset
  of $\mathcal{R}^k$, then the distribution of $(T_1, ..., T_k)$ is an exponential family
  of the form
  \begin{align*}
    f_T(u_1, ..., u_k | \theta)
    &=
    \\
    H(u_1, ..., u_k)
    \left[c(\theta)\right]^n &
      \exp{\left( \sum_{i = 1}^{k} w_i(\theta) u_i(x) \right)}
  \end{align*}
\end{theorem}

\begin{theorem}{5.3.1}
  Let $X_1, ..., X_n$ be a random sample from a $\mathcal{N}(\mu, \sigma^{2})$
  distribution, and let $\bar{X} = (1 / n) \sum_{i=1}^n X_i$ and
  $S^2 = [1 / (n - 1)] \sum_{i=1}^{n} (X_i - \bar{X})^{2}$.
  Then
  \begin{enumerate}[label=\alph*.]
    \item $\bar{X}$ and $S^{2}$ are independent random variables,
    \item $\bar{X}$ has a $\mathcal{N}(\mu, \sigma^{2}/n)$ distribution,
    \item $(n-1) S^{2} / \sigma^{2}$ has a chi squared distribution with $n - 1$ degrees of freedom.
  \end{enumerate}
\end{theorem}

\begin{lemma}{5.3.2}[Facts about chi squared random variables]
  We use the notation $\chi_p^2$ to denote a chi squared random variable with p degrees of freedom.
  \begin{enumerate}[label=\alph*.]
    \item If $Z$ is a $\mathcal{N}(0, 1)$ random variable, then $Z^{2} \sim \chi_1^2$;
      that is, the square of a standard normal random variable is a chi squared random variable.
    \item If $X_1, ..., X_n$ are independent and $X_i \sim \chi_{p_i}^2$, then
      $X_1 + ... + X_n \sim \chi_{p_1 + ... + p_n}^2$;
      that is, independent chi squared variables add to a chi squared variable, and the degrees
      also add.
  \end{enumerate}
\end{lemma}

\begin{lemma}{5.3.3}
  Let $X_j \sim \mathcal{N}(\mu_j, \sigma_j^2)$, $j = 1, ..., n$, independent.
  For constants $a_{ij}$ and $b_{rj}$, $(j = 1, ..., n; i = 1, ..., k; r = 1, ..., m)$,
  where $k + m \leq n$, define
  \begin{align*}
    U_i &= \sum_{j = 1}^{n} a_{ij} X_j, &i = 1, ..., k,
    \\
    V_r &= \sum_{j = 1}^{n} b_{rj} X_j, &r = 1, ..., m,
  \end{align*}
  \begin{enumerate}[label=\alph*.]
    \item The random variables $U_i$ and $V_r$ are independent if and only if $\Cov{U_i}{V_r} = 0$.
      Furthermore, $\Cov{U_i}{V_r} = \sum_{j=1}^n a_{ij}b_{rj} \sigma_j^2$.
    \item The random vectors $(U_1, ..., U_k)$ and $(V_1, ..., V_m)$ are independent if only if
      $U_i$ is independent of $V_r$ for all pairs $i, r$ $(i = 1, ..., k; r = 1, ..., m)$.
  \end{enumerate}
\end{lemma}

\begin{definition}{5.3.4}[Student's t distribution]
  Let $X_1$, ..., $X_n$ be a random sample from a $\mathcal{N}(\mu, \sigma^2)$ distribution.
  The quantity $(\bar{X} - \mu) / (S / \sqrt{n})$ has a \textit{Student's t distribution
  with $n-1$ degrees of freedom}. Equivalently, a random variable $T$ has a Student's t
  distribution with $p$ degrees of freedom, and we write $T \sim t_p$ if it has pdf
  \begin{equation*}
    f_T(t) = \frac{\Gamma(\frac{p + 1}{2})}{\Gamma(\frac{p}{2})}
    \frac{1}{\sqrt{p\pi}} \frac{1}{(1 + t^2/p)^{(p+1)/2}}, ~~ -\infty < t < \infty
  \end{equation*}
\end{definition}

\begin{definition}{5.3.6}[Snecedor's F distribution]
  Let $X_1$, ..., $X_n$ be a random sample from a $\mathcal{N}(\mu_X, \sigma_X^2)$ population,
  and let $Y_1, ..., Y_m$ be a random sample from an independent $\mathcal{N}(\mu_Y, \sigma_Y^2)$
  population.
  The random variable $F = (S_X^2 / \sigma_X^2) / (S_Y^2 / \sigma_Y^2)$ has a
  \textit{Snedecor's F distribution with $n - 1$ and $m - 1$ degrees of freedom}.
  Equivalently, a random variable $F$ has the $F$ distribution with $p$ and $q$
  degrees of freedom if it has pdf
  \begin{equation*}
    \begin{gathered}
      f_F(x) = \frac{\Gamma(\frac{p + q}{2})}{\Gamma(\frac{p}{2})\Gamma(\frac{q}{2})}
        \left( \frac{p}{q} \right)^{p/2}
        \frac{x^{(p/2) - 1}}{[1 + (p/q)x]^{(p+q)/2}},
        \\
        0 < x < \infty
    \end{gathered}
  \end{equation*}
\end{definition}


\begin{theorem}{5.3.8}
  Properties of the $t$ and $F$ distributions:
  \begin{enumerate}[label=\alph*.]
    \item If $X \sim F_{p, q}$, then $1 / X \sim F_{q,p}$; that is, the reciprocal of an $F$
      random variable is again an $F$ random variable.
    \item If $X \sim t_q$, then $X^2 \sim F_{1,q}$.
    \item If $X \sim F_{p,q}$, then $(p/q) X / (1 + (p/q)X) \sim \text{beta}(p/2, q/2)$.
  \end{enumerate}
\end{theorem}

\begin{definition}{5.5.1}[Convergence in probability]
  A sequence of random variables, $X_1, X_2, ...$, \textit{converges in probability}
  to a random variable $X$, if for every $\epsilon > 0$,
  \begin{equation*}
    \lim_{n \rightarrow \infty} P(|X_n - X| \geq \epsilon) = 0
  \end{equation*}
  or, equivalently,
  \begin{equation*}
    \lim_{n \rightarrow \infty} P(|X_n - X| < \epsilon) = 1.
  \end{equation*}
\end{definition}


\begin{theorem}{5.5.2}[Weak Law of Large Numbers]
  Let $X_1, X_2, ...$ be iid random variables with $\E{X_i} = \mu$ and $\Var{X_i} = \sigma^2 < \infty$.
  Define $\bar{X}_n = (1 / n) \sum_{i=1}^n X_i$. Then for every $\epsilon > 0$,
  \begin{equation*}
    \lim_{n \rightarrow \infty} P(|\bar{X}_n - \mu| < \epsilon) = 1;
  \end{equation*}
  that is, $\bar{X}_n$ converges in probability to $\mu$.
  This is known as \textit{consistency}.
\end{theorem}

\begin{theorem}{5.5.4}
  \label{theorem:5.5.4}
  Suppose that $X_1, X_2, ...$ converges in probability to a random variable $X$
  and that $h$ is a continuous function. Then $h(X_1), h(X_2), ...$ converges
  in probability to $h(X)$.

  If $S_n^2$ is a consistent estimator of $\sigma^2$, then by theorem \ref{theorem:5.5.4},
  the sample standard deviation $S_n = \sqrt{S_n^2} = h(S_n^2)$ is a consistent estimator
  for $\sigma$. Note that $S_n$ is, in fact, a biased estimator of $\sigma$,
  but the bias disappears asymptotically.
\end{theorem}

\begin{definition}{5.5.10}[Convergence in distribution]
  A sequence of random variables, $X_1, X_2, ...$, \textit{converges in distribution} 
  to a random variable $X$ if
  \begin{equation*}
    \lim_{n \rightarrow \infty} F_{X_n}(x) = F_X(x)
  \end{equation*}
  at all points $x$ where $F_X(x)$ is continuous.
\end{definition}

\begin{theorem}{5.5.12}
  If the sequence of random variables, $X_1, X_2, ...$, converges in probability
  to a random variable $X$, the sequence also converges in distribution to $X$.
\end{theorem}

\begin{theorem}{5.5.13}
  The sequence of random variables, $X_1, X_2, ...$, converges in probability to a
  constant $\mu$ if and only if the sequence also converges in distribution to $\mu$.
  That is, the statement
  \begin{equation*}
    \lim_{n \rightarrow \infty} P(|X_n - \mu| > \epsilon) \rightarrow 0
    \text{ for every } \epsilon > 0
  \end{equation*}
  is equivalent to
  \begin{equation*}
    P(X_n \leq x) \rightarrow \begin{cases}
      0 \text{ if } x < \mu \\
      1 \text{ if } x > \mu.
    \end{cases}
  \end{equation*}
\end{theorem}

\begin{theorem}{5.5.14}[Central Limit Theorem]
  $~$\\ Let $X_1, X_2, ...$ be a sequence of iid random variables whose mgfs exist in a neighborhood of $0$
  (that is, $M_{X_i}(t)$ exists for $|t| < h$, for some positive $h$).
  Let $\E{X_i} = \mu$ and $\Var{X_i} = \sigma^2 > 0$.
  (Both $\mu$ and $\sigma^2$ are finite since the mgf exists.)
  Define $\bar{X}_n = (1/n) \sum_{i=1}^n X_i$. Let $G_n(x)$ denote the cdf of $\sqrt{n} (\bar{X}_n - \mu)/\sigma$.
  Then, for any $x$, $-\infty < x < \infty$,
  \begin{equation*}
    \lim_{n \rightarrow \infty} G_n(x) = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}} e^{-y^2 / 2} \dif y
  \end{equation*}
  that is, $\sqrt{n} (\bar{X}_n - \mu) / \sigma$ has a limiting standard normal distribution.
\end{theorem}

\begin{theorem}{5.5.15}[Stronger form of the Central Limit Theorem]
  Let $X_1, X_2, ...$ be a sequence of iid random variables with $\E{X_i} = \mu$ and
  $0 < \Var{X_i} = \sigma^2 < \infty$.
  Define $\bar{X}_n = (1/n) \sum_{i=1}^n X_i$. Let $G_n(x)$ denote the cdf of $\sqrt{n} (\bar{X}_n - \mu)/\sigma$.
  Then, for any $x$, $-\infty < x < \infty$,
  \begin{equation*}
    \lim_{n \rightarrow \infty} G_n(x) = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}} e^{-y^2 / 2} \dif y
  \end{equation*}
  that is, $\sqrt{n} (\bar{X}_n - \mu) / \sigma$ has a limiting standard normal distribution.
\end{theorem}

\begin{theorem}{5.5.17}[Slutsky's Theorem]
  If $X_n \rightarrow X$ in distribution and $Y_n \rightarrow a$, a constant in probability, then
  \begin{enumerate}[label=\alph*.]
    \item $Y_nX_n \rightarrow aX$ in distribution.
    \item $Y_n + X_n \rightarrow X + a$ in distribution.
  \end{enumerate}
\end{theorem}

\begin{definition}{5.5.20}[Taylor polynomial]
  If a function $g(x)$ has derivatives of order $r$, that is, $g^{(r)}(x) = \od[r]{}{x} g(x)$ exists,
  then for any constant $a$, the \textit{Taylor polynomial of order $r$ about $a$ is} 
  \begin{equation*}
    T_r(x) = \sum_{i = 0}^{r} \frac{g^{(i)}(a)}{i!} (x - a)^i.
  \end{equation*}
\end{definition}

\begin{theorem}{5.5.21}[Taylor]
  If $g^{(r)}(a) = \eval{\od[r]{}{x} g(x)}_{x=a}$ \\ exists, then
  \begin{equation*}
    \lim_{x \rightarrow a} \frac{g(x) - T_r(x)}{(x - a)^r} = 0.
  \end{equation*}
\end{theorem}

\begin{theorem}{5.5.24}[Delta Method]
  Let $Y_n$ be a sequence of random variables that satisfies
  $\sqrt{n}(Y_n - \theta) \rightarrow \mathcal{N}(0, \sigma^2)$ in distribution.
  For a given function $g$ and a specific value of $\theta$, suppose that $g'(\theta)$ exists and is not $0$.
  Then
  \begin{equation*}
    \sqrt{n} \left[ g(Y_n) - g(\theta) \right]
    \rightarrow
    \mathcal{N}\left(0, \sigma^2 \left[ g'(\theta) \right]^2 \right)
  \end{equation*}
  in distribution.
\end{theorem}

\begin{theorem}{5.5.26}[Second-order Delta Method]
  Let $Y_n$ be a sequence of random variables that satisfies
  $\sqrt{n}(Y_n - \theta) \rightarrow \mathcal{N}(0, \sigma^2)$ in distribution.
  For a given function $g$ and a specific value of $\theta$, suppose that $g'(\theta) = 0$ and $g''(\theta)$ exists and is not $0$.
  Then
  \begin{equation*}
    n \left[ g(Y_n) - g(\theta) \right]
    \rightarrow
    \sigma^2 \frac{g''(\theta)}{2} \chi_1^2
  \end{equation*}
  in distribution.
\end{theorem}

\begin{theorem}{5.5.28}[Multivariate Delta Method]
  Let $\vec{X}_1, ..., \vec{X}_n$ be a random sample with $\E{X_{ij}} = \mu_i$ and $\Cov{X_{ik}}{X_{jk}} = \sigma_{ij}$.
  For a given function $g$ with continuous first partial derivatives and a specific value of $\vec{\mu} = (\mu_1, ..., \mu_p)$
  for which $\tau = \sum \sum \sigma_{ij} \pd{g(\vec{\mu})}{\mu_i} \cdot \pd{g(\vec{\mu})}{\mu_j} > 0$,
  \begin{equation*}
    \sqrt{n} \left[ g\left( \bar{X}_1, ..., \bar{X}_s \right) - g(\mu_1, ..., \mu_p) \right] \rightarrow \mathcal{N}(0, \tau^2)
  \end{equation*}
  in distribution.
\end{theorem}

\newpage
\section{\large{Principles of Data Reduction}}

\begin{definition}{6.2.0}[Sufficiency Principle]
  If $T(\vec{X})$ is a sufficient statistic for $\theta$, then any inference abouth $\theta$ should depend on the sample
  $\vec{X}$ only through the value $T(\vec{X})$. That is, if $\vec{x}$ and $\vec{y}$ are two sample points such that
  $T(\vec{x}) = T(\vec{y})$, then the inference about $\theta$ should be the same whether $\vec{X} = \vec{x}$
  or $\vec{X} = \vec{y}$ is observed.
\end{definition}

\begin{definition}{6.2.1}[Sufficient Statistic]
  A statistic $T(\vec{X})$ is a \textit{sufficient statistic for $\theta$} if the conditional distribution
  of the sample $\vec{X}$ given the value of $T(\vec{X})$ does not depend on $\theta$.
\end{definition}

\begin{theorem}{6.2.2}
  If $p(\vec{x} | \theta)$ is the joint pdf or pmf of $\vec{X}$ and $q(t|\theta)$ is the pdf or pmf of $T(\vec{X})$,
  then $T(\vec{X})$ is a sufficient statistic for $\theta$ if, for every $\vec{x}$ in the sample space,
  the ratio $p(\vec{x}|\theta) / q(T(\vec{x}) | \theta)$ is constant as a function of $\theta$.
\end{theorem}

\begin{theorem}{6.2.6}[Factorization Theorem]
  Let $f(\vec{x} | \theta)$ denote the joint pdf or pmf of a sample $\vec{X}$. A statistic $T(\vec{X})$ is a sufficient
  statistic for $\theta$ if and only if there exist functions $g(t|\theta)$ and $h(\vec{x})$ such that,
  for all sample points $\vec{x}$ and all parameter points $\theta$,
  \begin{equation*}
    f(\vec{x} | \theta) = g(T(\vec{x}) | \theta) h(\vec{x})
  \end{equation*}
\end{theorem}

\begin{theorem}{6.2.10}
  Let $X_1, ..., X_n$ be iid observations from a pdf or pmf $f(x|\theta)$ that belongs to an exponential family
  given by
  \begin{equation*}
    f(x|\vec{\theta}) = h(x)c(\vec{\theta}) \exp\left( \sum_{i = 1}^{k} w_i(\vec{\theta}) t_i(x) \right),
  \end{equation*}
  where $\vec{\theta} = (\theta_1, \theta_2, ..., \theta_d), d \leq k$. Then
  \begin{equation*}
    T(\vec{X}) = \left(
      \sum_{j = 1}^{n} t_1(X_j), ..., \sum_{j = 1}^{n} t_k(X_j)
    \right)
  \end{equation*}
  is a sufficient statistic for $\vec{\theta}$.
\end{theorem}

\begin{definition}{6.2.11}[Minimal Sufficient Statistic]
  A sufficient statistic $T(\vec{X})$ is called a \textit{minimal sufficient statistic} if,
  for any other sufficient statistic $T'(\vec{X})$, $T(\vec{x})$ is a function of $T'(\vec{x})$.
\end{definition}

\begin{theorem}{6.2.13}
  Let $f(\vec{x} | \theta)$ be the pmf or pdf of a sample $\vec{X}$. Suppose there exists a
  function $T(\vec{x})$ such that, for every two sample points $\vec{x}$ and $\vec{y}$,
  the ratio $f(\vec{x} | \theta) / f(\vec{y} | \theta)$ is constant as a function of
  $\theta$ if and only if $T(\vec{x}) = T(\vec{y})$. Then $T(\vec{X})$ is a minimial
  sufficient statistic for $\theta$.
\end{theorem}

\begin{definition}{6.2.21}[Complete Statistic]
  Let $f(t | \theta)$ be a family of pdfs or pmfs for a statistic $T(\vec{X})$.
  The family of probability distributions is called \textit{complete} if
  $\mathrm{\mathbb{E}}_{\theta} \left[ g(T) \right] = 0$ for all $\theta$
  implies $P_{\theta}(g(T) = 0) = 1$ for all $\theta$.
  Equivalently, $T(\vec{X})$ is called a \textit{complete statistic}.
\end{definition}

\begin{theorem}{6.2.25}[Complete Statistic in the Exponential Family]
  Let $X_1, ..., X_n$ be iid observations from an exponential family with pdf or pmf of the
  form
  \begin{equation*}
    f(x|\vec{\theta}) = h(x) c(\vec{\theta}) \exp \left(
      \sum_{j = 1}^{k} w(\theta_j) t_j(x)
    \right)
  \end{equation*}
  where $\vec{\theta} = (\theta_1, \theta_2, ..., \theta_k)$. Then the statistic
  \begin{equation*}
    T(\vec{X}) = \left(
      \sum_{i = 1}^{n} t_1(X_i), \sum_{i = 1}^{n} t_2(X_i), ..., \sum_{i = 1}^{n} t_k(X_i)
    \right)
  \end{equation*}
  is complete as long as the parameter space $\Theta$ contains an open set in $\mathcal{R}^k$.
\end{theorem}

\section{Point Estimation}

\begin{definition}{7.1.1}[Point estimator]
  A \textit{point estimator} is any function $W(X_1, ..., X_n)$ of a sample;
  that is, any statistic is a point estimator.
\end{definition}

\begin{definition}{7.2.0}[Method of moments]
  The \textit{j}th sample moment, $m_j$, and \textit{j}th population moment are respectively
  defined as.
  \begin{align*}
    m_j &= \frac{1}{n} \sum_{i = 1}^{n} X_i^j,
    \\
    \mu_j' &= \E{X^j}.
  \end{align*}
  The population moment $\mu_j'$ will typically be a function of $\theta_1, ..., \theta_k$,
  say $\mu_j'(\theta_1, ..., \theta_k)$.
  The method of moments involves solving the following system of equations for
  $(\theta_1, ..., \theta_k)$.
  \begin{equation*}
    m_j = \mu_j'(\theta_1, ..., \theta_k),~~~~j = 1, ..., k
  \end{equation*}
\end{definition}

\begin{definition}{7.2.4}[Maximum likelihood estimator]
  For each sample point $\vec{x}$, let $\hat{\theta}(\vec{x})$ be a parameter value at which
  $L(\theta|\vec{x})$ attains its maximum as a function of $\theta$, with $\vec{x}$ held fixed.
  A \textit{maximum likelihood estimator} (MLE) of the parameter $\theta$ based on a sample
  $\vec{X}$ is $\hat{\theta}(\vec{X})$.
\end{definition}

\begin{theorem}{7.2.10}[Invariance property of MLEs]
  If $\hat{\theta}$ is the MLE of $\theta$, then for any function $\tau(\theta)$, the MLE
  of $\tau(\theta)$ is $\tau(\hat{\theta})$.
\end{theorem}

\begin{definition}{7.2.5}[Conjugate family]
  Let $\mathcal{F}$ denote the class of pdfs or pmfs $f(x | \theta)$ (indexed by $\theta$).
  A class $\Pi$ of prior distributions is a \textit{conjugate family} for $\mathcal{F}$
  if the posterior distribution is in the class $\Pi$ for all $f \in \mathcal{F}$,
  all priors in $\Pi$, and all $x \in \mathcal{X}$.
\end{definition}

\begin{definition}{7.3.1}[Mean square error]
  The \textit{mean squared error} (MSE) of an estimator $W$ of a parameter $\theta$
  is the function of $\theta$ defined by $\mathbb{E}_{\theta}\left[ (W - \theta)^2 \right]$.
\end{definition}

\begin{definition}{7.3.2}[Bias]
  The \textit{bias} of a point estimator $W$ of a parameter $\theta$ is the difference
  between the expected value of $W$ and $\theta$; that is,
  $\mathrm{Bias}_{\theta}(W) = \mathbb{E}_{\theta}\left[ W \right] - \theta$.
  An estimator whose bias is identically (in $\theta$) equal to $0$ is called
  \textit{unbiased} and satisfies $\mathbb{E}_{\theta} \left[ W \right] = \theta$
  for all $\theta$.
\end{definition}

\begin{definition}{7.3.7}[Best unbiased estimator]
  \label{theorem:7.3.7}
  An estimator $W^*$ is a \textit{best unbiased estimator} of $\tau(\theta)$ if it satisfies
  $\mathbb{E}_{\theta} \left[ W^* \right] = \tau(\theta)$ for all $\theta$ and, for any
  other estimator $W$ with $\mathbb{E}_{\theta} \left[ W \right] = \tau(\theta)$,
  we have $\mathrm{Var}_{\theta} \left(W^*\right) \leq \mathrm{Var}_{\theta} \left(W\right)$
  for all $\theta$. $W^*$ is also called a \textit{uniform minimum variance unbiased estimator} 
  (UMVUE) of $\tau(\theta)$.
\end{definition}

\begin{theorem}{7.3.9}[Cramér-Rao Inequality]
  Let $X_1, ..., X_n$ be a sample with pdf $f(\vec{x} | \theta)$, and let
  $W(\vec{X}) = W(X_1, ..., X_n)$ be any estimator satisfying
  \begin{equation*}
    \od{}{\theta} \mathbb{E}_{\theta} \left[ W(\vec{X}) \right]
    =
    \int_{\mathcal{X}} \pd{}{\theta} \left[ W(\vec{x}) f(\vec{x} | \theta) \right] \dif{\vec{x}}
  \end{equation*}
  and
  \begin{equation*}
    \mathrm{Var}_{\theta} \left( W(\vec{X}) \right) < \infty.
  \end{equation*}
  Then
  \begin{equation*}
    \mathrm{Var}_{\theta} \left( W(\vec{X}) \right)
    \geq
    \frac{
      \left( \od{}{\theta} \mathbb{E}_{\theta} \left[ W(\vec{X}) \right] \right)^2
    }{
      \mathbb{E}_{\theta} \left[
        \left( \pd{}{\theta} \log f(\vec{X} | \theta) \right)^2
      \right]
    }
  \end{equation*}
\end{theorem}

\begin{corollary}{7.3.10}[Cramér-Rao Inequality, iid case]
  If the assumptions of Theorem \ref{theorem:7.3.7} are satisfied and, additionally,
  if $X_1, ..., X_n$ are iid with pdf $f(x|\theta)$, then
  \begin{equation*}
    \mathrm{Var}_{\theta} \left( W(\vec{X}) \right)
    \geq
    \frac{
      \left( \od{}{\theta} \mathbb{E}_{\theta} \left[ W(\vec{X}) \right] \right)^2
    }{
      n \mathbb{E}_{\theta} \left[
        \left( \pd{}{\theta} \log f(X | \theta) \right)^2
      \right]
    }
  \end{equation*}
\end{corollary}

\begin{lemma}{7.3.11}
  If $f(x | \theta)$ satisfies
  \begin{gather*}
    \od{}{\theta} \mathbb{E}_{\theta} \left( \pd{}{\theta} \log f(X | \theta) \right)
    \\
    =
    \int \pd{}{\theta} \left[
      \left( \pd{}{\theta} \log(f(x | \theta)) \right) f(x | \theta)
    \right] \dif{x}
  \end{gather*}
  (true for an exponential family), then
  \begin{equation*}
    \mathbb{E}_{\theta} \left[ \left(
        \pd{}{\theta} \log (f(X|\theta))
    \right)^2 \right]
    =
    - \mathbb{E}_{\theta} \left[
      \pd[2]{}{\theta} \log (f(X|\theta))
    \right]
  \end{equation*}
\end{lemma}

\begin{corollary}{7.3.15}[Attainment]
  Let $X_1, ..., X_n$ be iid $f(x|\theta)$, where $f(x|\theta)$ satisfies the conditions
  of the Cramér-Rao Theroem. Let $L(\theta|\vec{x}) = \prod_{i=1}^{n} f(x_i | \theta)$
  denote the likelihood function. If $W(\vec{X}) = W(X_1, ..., X_n)$ is any unbiased
  estimator of $\tau(\theta)$, then $W(\vec{X})$ attains the Cramér-Rao Lower Bound
  if and only if
  \begin{equation*}
    a(\theta) \left[ W(\vec{x}) - \tau(\theta) \right]
    =
    \pd{}{\theta} \log (L(\theta|\vec{x}))
  \end{equation*}
  for some function $a(\theta)$.
\end{corollary}

\begin{theorem}{7.3.17}[Rao-Blackwell]
  Let $W$ be any unbiased estimator of $\tau(\theta)$, and let $T$ be a sufficient statistic
  for $\theta$. Define $\phi(T) = \E{W | T}$. Then $\mathbb{E}_{\theta}[\phi(T)] = \tau(\theta)$
  and $\mathrm{Var}_{\theta}(\phi(T)) \leq \mathrm{Var}_{\theta}(W)$ for all $\theta$;
  that is, $\phi(T)$ is a uniformly better unbiased estimator of $\tau(\theta)$.
\end{theorem}

\begin{theorem}{7.3.23}
  Let $T$ be a complete sufficient statistic for a parameter $\theta$, and let $\phi(T)$
  be any estimator based only on $T$. Then $\phi(T)$ is the unique best unbiased estimator
  of its expected value.
\end{theorem}

\begin{theorem}{7.5.1}[Lehmann-Scheffé]
  Unbiased estimators based on complete sufficient statistics are unique.
\end{theorem}

\section{Hypothesis Testing}

\begin{definition}{8.1.1}[Hypothesis]
  A \textit{hypothesis} is a statement about a population parameter.
\end{definition}

\begin{definition}{8.1.2}[Null hypothesis]
  The complementary hypotheses in a hypothesis testing problem are called the
  \textit{null hypothesis} and the \textit{alternative hypothesis}.
  They are donetd by $H_0$ and $H_1$, respectively.
\end{definition}

\begin{definition}{8.1.3}[Hypothesis testing procedure]
  A \textit{hypothesis testing procedure} or \textit{hypothesis test}
  is a rule that specifies
  \begin{enumerate}[label=\roman*.]
    \item For which sample values the decision is made to accept $H_0$ as true.
    \item For which sample values $H_0$ is rejected and $H_1$ is accepted as true.
  \end{enumerate}
  The subset of the sample space for which $H_0$ will be rejected is called the
  \textit{rejection region} or \textit{critical region}. The complement of the
  rejection region is called the \textit{acceptance region}.
\end{definition}

\begin{definition}{8.2.1}[Likelihood ratio test statistic]
  The \textit{likelihood ratio test statistic} for testing $H_0: \theta \in \Theta_0$
  versus $H_1: \theta \in \Theta_0^c$ is
  \begin{equation*}
    \lambda(\vec{x})
    =
    \frac{
      \sup \limits_{\Theta_0} L(\theta | \vec{x})
    }{
      \sup \limits_{\Theta} L(\theta | \vec{x})
    }
  \end{equation*}
  A \textit{likelihood ratio test} (LRT) is any test that has a rejection region
  of the form $\{\vec{x}: \lambda(\vec{x}) \leq c \}$, where $c$ is any number
  satisfying $0 \leq c \leq 1$.
\end{definition}

\begin{theorem}{8.2.4}
  If $T(\vec{X})$ is a sufficient statistic for $\theta$ and $\lambda^*(t)$ and
  $\lambda(\vec{x})$ are the LRT statistics based on $T$ and $\vec{X}$, respectively,
  then $\lambda^*(T(\vec{x})) = \lambda(\vec{x})$ for every $\vec{x}$ in the sample space.
\end{theorem}

\begin{definition}{8.3.1}[Power function]
  The \textit{power function} of a hypothesis test with rejection region $R$ is the function
  of $\theta$ defined by $\beta(\theta) = P_{\theta}(\vec{X} \in R)$.
\end{definition}

\begin{definition}{8.3.5}[Size $\alpha$ test]
  For $0 \leq \alpha \leq 1$, a test with power function $\beta(\theta)$ is a
  \textit{size $\alpha$ test} if $\sup_{\theta \in \Theta_0} \beta(\theta) = \alpha$.
\end{definition}

\begin{definition}{8.3.6}[Level $\alpha$ test]
  For $0 \leq \alpha \leq 1$, a test with power function $\beta(\theta)$ is a
  \textit{level $\alpha$ test} if $\sup_{\theta \in \Theta_0} \beta(\theta) \leq \alpha$.
\end{definition}

\begin{definition}{8.3.11}[Uniformly most powerful class $\mathcal{C}$ test]
  Let $\mathcal{C}$ be a class of tests for testing $H_0: \theta \in \Theta_0$
  versus $H_1: \theta \in \Theta_0^c$. A test in class $\mathcal{C}$, with power function
  $\beta(\theta)$, is a \textit{uniformly most powerful} (UMP) \textit{class $\mathcal{C}$ test} 
  if $\beta(\theta) \geq \beta^{\prime}(\theta)$ for every $\theta \in \Theta_0^c$ and every
  $\beta^{\prime}(\theta)$ that is a power function of a test in class $\mathcal{C}$.
\end{definition}

\begin{theorem}{8.3.12}[Neyman-Pearson Lemma]
  \label{theorem:8.3.12}
  Consider testing $H_0: \theta \in \theta_0$ versus $H_1: \theta \in \theta_1$, where the pdf or pmf
  corresponding to $\theta_i$ is $f(\vec{x} | \theta_i), i = 0, 1$, using a test with rejection region
  $R$ that satisfies
  \begin{align*}
    &\vec{x} \in R &\text{ if } f(\vec{x} | \theta_1) > k f(\vec{x} | \theta_0)
    \\
    &\vec{x} \in R^c &\text{ if } f(\vec{x} | \theta_1) < k f(\vec{x} | \theta_0)
  \end{align*}
  for some $k \geq 0$, and
  \begin{equation*}
    \alpha = P_{\theta_0}(\vec{X} \in R)
  \end{equation*}
  Then
  \begin{enumerate}[label=\alph*.]
    \item (Sufficiency) Any test that satisfies the two requirements above is a UMP level $\alpha$ test.
  \end{enumerate}
\end{theorem}

\begin{corollary}{8.3.13}
  Consider the hypothesis problem posed in Theorem \ref{theorem:8.3.12}. Suppose $T(\vec{X})$ is a sufficient
  statistic for $\theta$ and $g(t | \theta_i)$ is the pdf or pmf of $T$ corresponding to $\theta_i, i = 0, 1$.
  Then any test based on $T$ with rejection region $S$ (a subset of the sample space of $T$) is a UMP level
  $\alpha$ test if it satisfies
  \begin{align*}
    &t \in S &\text{ if }~~ g(t | \theta_1) > k g(t|\theta_0)
    \\
    &t \in S^c &\text{ if }~~ g(t | \theta_1) < k g(t|\theta_0)
  \end{align*}
  for some $k \geq 0$, where
  \begin{equation*}
    \alpha = P_{\theta_0}(T \in S).
  \end{equation*}
\end{corollary}

\section{Interval Estimation}

\begin{definition}{9.1.1}[Interval estimate]
  An \textit{interval estimate} of a real-valued parameter $\theta$ is any pair of functions,
  $L(x_1, ..., x_n)$ and $U(x_1, ..., x_n)$, of a sample that satisfy $L(\vec{x}) \leq U(\vec{x})$
  for all $\vec{x} \in \mathcal{X}$. If $\vec{X} = \vec{x}$ is observed, the inference
  $L(\vec{x}) \leq \theta \leq U(\vec{x})$ is made. The random interval $[L(\vec{X}), U(\vec{X})]$
  is called an \textit{interval estimator}.
\end{definition}

\begin{definition}{9.1.4}[Coverage probability]
  For an interval estimator $[L(\vec{X}), U(\vec{X})]$ of a parameter $\theta$, the \textit{coverage probability} 
  of $[L(\vec{X}), U(\vec{X})]$ is the probability that the random interval $[L(\vec{X}), U(\vec{X})]$ covers the true
  parameter, $\theta$. In symbols, it is doneted by either $P_{\theta}(\theta \in [L(\vec{X}), U(\vec{X})])$
  or  $P(\theta \in [L(\vec{X}), U(\vec{X})] | \theta)$.
\end{definition}

\begin{definition}{9.1.5}[Confidence coefficient]
  For an interval estimator $[L(\vec{X}), U(\vec{X})]$ of a parameter $\theta$, the \textit{confidence coefficient}
  of $[L(\vec{X}), U(\vec{X})]$ is the infimum of the coverage probabilities,
  $\inf_{\theta} P_{\theta}(\theta \in [L(\vec{X}), U(\vec{X})])$.
\end{definition}

\begin{theorem}{9.2.2}
  For each $\theta_0 \in \Theta$, let $A(\theta_0)$ be the acceptance region of a level $\alpha$ test of
  $H_0: \theta = \theta_0$. For each $\vec{x} \in \mathcal{X}$, define a set $C(\vec{x})$ in the parameter space by
  \begin{equation*}
    C(\vec{x}) = \{ \theta_0: \vec{x} \in A(\theta_0) \}.
  \end{equation*}
  Then the random set $C(\vec{X})$ is a $1 - \alpha$ confidence set. Conversely, let $C(\vec{X})$ be a $1 - \alpha$
  confidence set. For any $\theta_0 \in \Theta$, define
  \begin{equation*}
    A(\theta_0) = \{ \vec{x}: \theta_0 \in C(\vec{x}) \}.
  \end{equation*}
  Then $A(\theta_0)$ is the acceptance region of a level $\alpha$ test of $H_0: \theta = \theta_0$.
\end{theorem}

\begin{definition}{9.2.6}[Pivotal quantity]
  A random variable $Q(\vec{X}, \theta) = Q(X_1, ..., X_n, \theta)$ is a \textit{pivotal quantity} (or \textit{pivot})
  if the distribution of $Q(\vec{X}, \theta)$ is independent of all parameters.
  That is, if $\vec{X} \sim  F(\vec{x} | \theta)$, then $Q(\vec{X} | \theta)$ has the same distribution for all values
  of $\theta$.
\end{definition}

\begin{theorem}{9.2.12}[Pivoting a continous cdf]
  Let $T$ be a statistic with continous cdf $F_T(t | \theta)$. Let $\alpha_1 + \alpha_2 = \alpha$ with $0 < \alpha < 1$
  be fixed values. Suppose that for each $t \in \mathcal{T}$, the functions $\theta_L(t)$ and $\theta_U(t)$
  can be defined as follows.
  \begin{enumerate}[label=\roman*.]
    \item If $F_T(t | \theta)$ is a decreasing function of $\theta$ for each $t$, define $\theta_L(t)$ and $\theta_U(t)$
      by
      \begin{equation*}
        F_T\left(t | \theta_U(t)\right) = \alpha_1,
        ~~~
        F_T\left(t | \theta_L(t)\right) = 1 - \alpha_2.
      \end{equation*}
    \item If $F_T(t | \theta)$ is an increasing function of $\theta$ for each $t$, define $\theta_L(t)$ and $\theta_U(t)$
      by
      \begin{equation*}
        F_T\left(t | \theta_U(t)\right) = 1 - \alpha_2,
        ~~~
        F_T\left(t | \theta_L(t)\right) = \alpha_1.
      \end{equation*}
  \end{enumerate}
  Then the random interval $[\theta_L(T),  \theta_U(T)]$ is a $1 - \alpha$ confidence interval for $\theta$.
\end{theorem}

\begin{theorem}{9.2.14}[Pivoting a discrete cdf]
  Let $T$ be a discrete statistic with cdf $F_T(t | \theta) = P(Tt \leq t | \theta)$. Let $\alpha_1 + \alpha_2 = \alpha$
  with  $0 < \alpha < 1$ be fixed values. Suppose that for each $t \in \mathcal{T}$, $\theta_L(t)$ and $\theta_U(t)$
  can be defined as follows.
  \begin{enumerate}[label=\roman*.]
    \item If $F_T(t | \theta)$ is a decreasing function of $\theta$ for each $t$, define $\theta_L(t)$ and $\theta_U(t)$
      by
      \begin{equation*}
        P\left(T \leq t | \theta_U(t)\right) = \alpha_1,
        ~~~
        P\left(T \geq | \theta_L(t)\right) = \alpha_2.
      \end{equation*}
    \item If $F_T(t | \theta)$ is an increasing function of $\theta$ for each $t$, define $\theta_L(t)$ and $\theta_U(t)$
      by
      \begin{equation*}
        P\left(T \geq t | \theta_U(t)\right) = \alpha_1,
        ~~~
        P\left(T \leq | \theta_L(t)\right) = \alpha_2.
      \end{equation*}
  \end{enumerate}
  Then the random interval $[\theta_L(T),  \theta_U(T)]$ is a $1 - \alpha$ confidence interval for $\theta$.
\end{theorem}

\section{Asymptotic Evaluations}

\begin{definition}{10.1.1}[Consistent sequence of estimators]
  A sequence of estimators $W_n = W_n(X_1, ..., X_n)$ is a \textit{consistent sequence of estimators}
  of the parameter $\theta$ if, for every $\epsilon > 0$ and every $\theta \in \Theta$,
  \begin{equation*}
    \lim \limits_{n \rightarrow \infty} P_{\theta}( |W_n - \theta| < \epsilon) = 1.
  \end{equation*}
\end{definition}

\begin{theorem}{10.1.3}
  If $W_n$ is a sequence of estimators of a parameter $\theta$ satisfying
  \begin{enumerate}[label=\roman*.]
    \item $\lim_{n \rightarrow \infty} \mathrm{Var}_{\theta} \left(W_n\right) = 0$,
    \item $\lim_{n \rightarrow \infty} \mathrm{Bias}_{\theta} \left(W_n\right) = 0$,
  \end{enumerate}
  for every $\theta \in \Theta$, then $W_n$ is a consistent sequence of estimators of $\theta$.
\end{theorem}

\begin{theorem}{10.1.5}
  Let $W_n$ be a consistent sequence of estimators of a parameter $\theta$.
  Let $a_1, a_2, ...$ and $b_1, b_2, ...$ be sequences of constants satisfying
  \begin{enumerate}[label=\roman*.]
    \item $\lim_{n \rightarrow \infty} a_n = 1$,
    \item $\lim_{n \rightarrow \infty} b_n = 0$.
  \end{enumerate}
  Then the sequence $U_n = a_n W_n + b_n$ is a consistent sequence of estimators of $\theta$.
\end{theorem}

\begin{theorem}{10.1.6}[Consistency of MLEs]
  Let $X_1, X_2, ...,$ be iid $f(x|\theta)$, and let $L(\theta | \vec{x}) = \prod_{i=1}^{n} f(x_i | \theta)$
  be the likelihood function. Let $\hat{\theta}$ denote thee MLE of $\theta$. Let $\tau(\theta)$ be a
  continous function of $\theta$. Under teh regularity conditions of Miscellanea 10.6.2 on $f(x | \theta)$
  and, hence, $L(\theta | \vec{x})$, for every $\epsilon > 0$ and every $\theta \in \Theta$,
  \begin{equation*}
    \lim \limits_{n \rightarrow \infty} P_{\theta}( |\tau(\hat{\theta}) - \tau(\theta)| \geq \epsilon) = 0.
  \end{equation*}
  That is, $\tau(\hat{\theta})$ is a consistent estimator of $\tau(\theta)$.
\end{theorem}

\begin{definition}{10.1.7}[Limiting variance]
  For an estimator $T_n$, if $\lim_{n \rightarrow \infty} k_n \Var{T_n} = \tau^2 < \infty$, where
  $\{k_n\}$ is a sequence of constants, then $\tau^2$ is called the \textit{limiting variance} or
  \textit{limit of the variances}.
\end{definition}

\begin{definition}{10.1.9}[Asymptotic variance]
  For an estimator $T_n$, suppose that $k_n(T_n - \tau(\theta)) \rightarrow \mathcal{N}(0, \sigma^2)$
  in distribution. The parameter $\sigma^2$ is called the \textit{asymptotic variance} or
  \textit{variance of the limit distribution} of $T_n$.
\end{definition}

\begin{definition}{10.1.11}[Asymptotically efficient]
  A sequence of estimators $W_n$ is \textit{asymptotically efficient} for a parameter $\tau(\theta)$
  if $\sqrt{n} [W_n - \tau(\theta)] \rightarrow  \mathcal{N}(0, v(\theta))$ in distribution and
  \begin{equation*}
    v(\theta) = \frac{
      \left[ \tau^{\prime}(\theta) \right]^2
    }{
      \mathrm{E}_{\theta} \left[ \left( \pd{}{\theta} \log \left(f(X | \theta)\right) \right)^2 \right]
    };
  \end{equation*}
  that is, the asymptotic variance of $W_n$ achieves the Cramér-Rao Lower Bound.
\end{definition}

\begin{theorem}{10.1.12}[Asymptotic efficiency of MLEs]
  Let $X_1, X_2, ...,$ be iid $f(x|\theta)$, let $\hat{\theta}$ denote the MLE of $\theta$, and let
  $\tau(\theta)$ be a continous function of $\theta$. Under the regularity conditions of Miscellanea
  10.6.2 on $f(x|\theta)$ and, hence, $L(\theta|\vec{x})$,
  \begin{equation*}
    \sqrt{n} \left[ \tau(\hat{\theta}) - \tau(\theta) \right]
    \rightarrow
    \mathcal{N} \left( 0, v(\theta) \right)
  \end{equation*}
  where $v(\theta)$ is the Cramér-Rao Lower Bound. That is, $\tau(\hat{\theta})$ is a consistent
  and asymptotically efficient estimator of $\tau(\theta)$.
\end{theorem}

\begin{definition}{10.1.16}[Asymptotic relative efficiency]
  If two estimators $W_n$ and $V_n$ satisfy
  \begin{gather*}
    \sqrt{n} \left[ W_N - \tau(\theta) \right] \rightarrow \mathcal{N} \left( 0, \sigma_W^2 \right)
    \\
    \sqrt{n} \left[ V_N - \tau(\theta) \right] \rightarrow \mathcal{N} \left( 0, \sigma_V^2 \right)
  \end{gather*}
  in distribution, the \textit{asymptotic relative efficiency} (ARE) of $V_n$ with respect to
  $W_n$ is
  \begin{equation*}
    \mathrm{ARE}(V_n, W_n) = \frac{\sigma_W^2}{\sigma_V^2}.
  \end{equation*}
\end{definition}

\begin{theorem}{10.3.1}[Asymptotic distribution of the LRT --- Simple $H_0$]
  For testing $H_0: \theta = \theta_0$ versus $H_1: \theta \neq \theta_0$, suppose $X_1, ..., X_n$
  are iid $f(x | \theta)$, $\hat{\theta}$ is the MLE of $\theta$, and $f(x | \theta)$ satisfies
  the regularity conditions of Miscellanea 10.6.2. Then under $H_0$, as $n \rightarrow \infty$,
  \begin{equation*}
    -2 \log \left( \lambda(\vec{X}) \right) \rightarrow \chi_1^2, \text{ in distribution,}
  \end{equation*}
  where $\chi_1^2$ is a $\chi^2$ random variable with 1 degree of freedom.
\end{theorem}

\begin{definition}{}[Score statistic]
  The \textit{score statistic} is defined to be
  \begin{equation*}
    S(\theta)
    =
    \pd{}{\theta} \log \left( f(\vec{X} | \theta) \right)
    =
    \pd{}{\theta} \log \left( L(\theta | \vec{X}) \right)
  \end{equation*}
\end{definition}
  
\newpage

\newpage
\appendix

\section{Appendix}

\subsubsection*{Binomial Coefficient Identities}

\begin{align}
  \binom{n}{x} &= \frac{n!}{x!(n-x)!}
  \\
  x \binom{n}{x} &= n \binom{n - 1}{x - 1}
  \\
  \binom{n}{x} &= \frac{n}{x} \binom{n - 1}{x - 1}
\end{align}

\subsubsection*{Gamma function}

\begin{align}
  \Gamma(\alpha + 1) &= \alpha \Gamma(\alpha) &\alpha > 0
  \\
  \Gamma(n) &= (n - 1)! & n \text{ positive integer}
  \\
  \Gamma \left( \frac{1}{2} \right) &= \sqrt{\pi}
\end{align}

\subsubsection*{Distribution of the variance estimator}
Let $Z_1, Z_2, ..., Z_n$ be $\mathcal{N}(\mu, \sigma^{2})$
and let
\[
  S^2 = \frac{1}{n-1} \sum_{i = 1}^{n} (Z_i - \bar{Z})^{2},
\]
be the variance estimator.
Then
\[
  \frac{(n-1)S^2}{\sigma^{2}} \sim \chi^2_{n-1}.
\]

\end{multicols}
\end{document}
